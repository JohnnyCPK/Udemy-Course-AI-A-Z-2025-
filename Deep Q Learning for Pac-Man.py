# -*- coding: utf-8 -*-
"""Copy of Deep Convolutional Q-Learning for Pac-Man - Partial Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SCEO4SrFLH-JwvFvzJqJWVED7A_aGkAP

# Deep Convolutional Q-Learning for Pac-Man

## Part 0 - Installing the required packages and importing the libraries

### Installing Gymnasium
"""

!pip install gymnasium
!pip install "gymnasium[atari, accept-rom-license]"
!apt-get install -y swig
!pip install gymnasium[box2d]

"""### Importing the libraries"""

import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
from torch.utils.data import DataLoader, TensorDataset

"""## Part 1 - Building the AI

### Creating the architecture of the Neural Network
"""

class Network(nn.Module):
    def __init__(self, action_size, seed=42):
        # Initialize the base class (nn.Module)
        super(Network, self).__init__()

        # Set the random seed for reproducibility
        self.seed = torch.manual_seed(seed)

        # Define the first convolutional layer
        # Input: 3 channels (e.g., RGB image), Output: 32 channels
        # Kernel size: 8x8, Stride: 4
        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)
        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization for the first layer

        # Define the second convolutional layer
        # Input: 32 channels, Output: 64 channels
        # Kernel size: 4x4, Stride: 2
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization for the second layer

        # Define the third convolutional layer
        # Input: 64 channels, Output: 64 channels
        # Kernel size: 3x3, Stride: 1
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)  # Batch normalization for the third layer

        # Define the fourth convolutional layer
        # Input: 64 channels, Output: 128 channels
        # Kernel size: 3x3, Stride: 1
        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)
        self.bn4 = nn.BatchNorm2d(128)  # Batch normalization for the fourth layer

        # Define the first fully connected (dense) layer
        # Input size: 10 * 10 * 128 (flattened feature maps after conv layers)
        # Output size: 512
        self.fc1 = nn.Linear(10 * 10 * 128, 512)

        # Define the second fully connected layer
        # Input size: 512, Output size: 256
        self.fc2 = nn.Linear(512, 256)

        # Define the third fully connected layer (output layer)
        # Input size: 256, Output size: action_size (number of possible actions)
        self.fc3 = nn.Linear(256, action_size)

    def forward(self, state):  # state is the input images
        # Pass the input through the first convolutional layer, followed by batch normalization and ReLU activation
        x = F.relu(self.bn1(self.conv1(state)))

        # Pass through the second convolutional layer, followed by batch normalization and ReLU activation
        x = F.relu(self.bn2(self.conv2(x)))

        # Pass through the third convolutional layer, followed by batch normalization and ReLU activation
        x = F.relu(self.bn3(self.conv3(x)))

        # Pass through the fourth convolutional layer, followed by batch normalization and ReLU activation
        x = F.relu(self.bn4(self.conv4(x)))

        # Flatten the output from the convolutional layers to pass into fully connected layers
        # x.size(0) is the batch size; -1 means flatten all other dimensions
        x = x.view(x.size(0), -1)

        # Pass through the first fully connected layer with ReLU activation
        x = F.relu(self.fc1(x))

        # Pass through the second fully connected layer with ReLU activation
        x = F.relu(self.fc2(x))

        # Pass through the output layer (fc3) to produce the final output (action scores or probabilities)
        return self.fc3(x)

"""## Part 2 - Training the AI

### Setting up the environment
"""

import gymnasium as gym

# Create the Ms. Pacman environment with deterministic behavior
# full_action_space=False limits the action space to a smaller set of actions
env = gym.make('MsPacmanDeterministic-v0', full_action_space=False)

# Get the shape of the state space
# This represents the dimensions of the input image from the game
state_shape = env.observation_space.shape

# Get the size of the state, typically the height of the image frame
state_size = env.observation_space.shape[0]

# Get the number of possible actions in this environment
# This is the number of discrete actions the agent can take
number_actions = env.action_space.n

# Print out the state shape, state size, and number of actions
print('State Shape: ', state_shape)       # e.g., (210, 160, 3) for an RGB image frame
print('State Size: ', state_size)         # The first dimension of the state shape, e.g., 210
print('Number of Actions: ', number_actions)  # e.g., 9 actions

"""### Initializing the hyperparameters"""

# Set the learning rate for the optimizer
# The learning rate determines how much to adjust the model's weights with respect to the loss gradient during each update.
# A smaller learning rate means smaller, more precise steps, while a larger rate means faster learning but can risk overshooting the minimum.
learning_rate = 5e-4  # 0.0005

# Set the size of each mini-batch of experiences
# During training, the agent will update the network using a random sample of this many experiences from the replay buffer.
# Using a minibatch helps in making the learning process more stable by averaging out noisy updates.
minibatch_size = 64

# Set the discount factor for future rewards
# This value determines the importance of future rewards in the agent's decision-making process.
# A value closer to 1 makes the agent consider future rewards more, encouraging long-term planning.
# A value closer to 0 makes the agent focus more on immediate rewards.
discount_factor = 0.99

"""### Preprocessing the frames"""

from PIL import Image
from torchvision import transforms

def preprocess_frame(frame):
    # Convert the frame (which is likely a NumPy array) into a PIL Image.
    # The frame is usually in the form of a NumPy array (H, W, C) where H is height, W is width, and C is the number of color channels.
    frame = Image.fromarray(frame)

    # Define a sequence of transformations to preprocess the image.
    # 1. Resize the image to 128x128 pixels.
    # 2. Convert the image to a PyTorch tensor and normalize pixel values to [0, 1] range.
    preprocess = transforms.Compose([
        transforms.Resize((128, 128)),  # Resizes the image to 128x128 pixels.
        transforms.ToTensor()           # Converts the PIL Image to a PyTorch tensor.
    ])

    # Apply the preprocessing transformations to the frame.
    # The 'unsqueeze(0)' adds an extra dimension to the tensor, making it (1, C, H, W),
    # where 1 is the batch size, C is the number of channels, H is the height, and W is the width.
    return preprocess(frame).unsqueeze(0)

"""### Implementing the DCQN class"""

class Agent():
    def __init__(self, action_size):
        # Set the device to GPU if available, otherwise use CPU
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        # Store the action size (number of possible actions)
        self.action_size = action_size

        # Initialize the local Q-network
        # This is the network that will be trained and used for selecting actions
        self.local_qnetwork = Network(action_size).to(self.device)

        # Initialize the target Q-network
        # This network is used to compute target values for the training process
        self.target_qnetwork = Network(action_size).to(self.device)

        # Set up the optimizer for the local Q-network using Adam optimizer
        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)

        # Initialize the replay memory as a deque with a maximum length of 1000
        # This will store the agent's experiences for experience replay
        self.memory = deque(maxlen=1000)

    def step(self, state, action, reward, next_state, done):
        # Preprocess the current and next state frames
        state = preprocess_frame(state)
        next_state = preprocess_frame(next_state)

        # Store the experience (state, action, reward, next_state, done) in replay memory
        self.memory.append((state, action, reward, next_state, done))

        # If enough experiences are stored, start learning from a random sample
        if len(self.memory) > minibatch_size:
            # Sample a random minibatch of experiences from memory
            experiences = random.sample(self.memory, k=minibatch_size)

            # Learn from the sampled experiences
            self.learn(experiences, discount_factor)

    def act(self, state, epsilon=0.):
        # Preprocess the state and move it to the appropriate device
        state = preprocess_frame(state).to(self.device)

        # Set the local Q-network to evaluation mode to prevent training updates
        self.local_qnetwork.eval()
        with torch.no_grad():
            # Get action values from the local Q-network
            action_values = self.local_qnetwork(state)
        # Set the network back to training mode
        self.local_qnetwork.train()

        # Epsilon-greedy action selection
        # With probability epsilon, select a random action for exploration
        if random.random() > epsilon:
            # Choose the action with the highest value (greedy action)
            return np.argmax(action_values.cpu().data.numpy())
        else:
            # Randomly choose an action from the action space
            return random.choice(np.arange(self.action_size))

    def learn(self, experiences, discount_factor):
        # Extract the components of each experience from the minibatch
        # The experiences are stored as tuples, so we use zip(*) to unpack them into separate arrays
        states, actions, rewards, next_states, dones = zip(*experiences)

        # Convert the experiences into tensors and move them to the appropriate device
        # Stack the states, actions, rewards, next states, and dones into batch tensors
        states = torch.from_numpy(np.vstack(states)).float().to(self.device)
        actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)
        rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)
        next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)
        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)

        # Get the max predicted Q values from the target Q-network for next states
        # Detach the network to stop gradient computation, as we don't want to update the target network
        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)

        # Compute the Q targets for the current states
        # Q targets are the rewards plus the discounted max Q values of the next states
        q_targets = rewards + discount_factor * next_q_targets * (1 - dones)

        # Get the expected Q values from the local Q-network for the chosen actions
        # The .gather method selects the Q values corresponding to the actions taken
        q_expected = self.local_qnetwork(states).gather(1, actions)

        # Compute the loss using Mean Squared Error (MSE) loss
        loss = F.mse_loss(q_expected, q_targets)

        # Perform backpropagation to update the local Q-network's parameters
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

"""### Initializing the DCQN agent"""

agent = Agent(number_actions)

"""### Training the DCQN agent"""

number_episodes = 2000
maximum_number_timesteps_per_episode = 10000
epsilon_starting_value = 1.0
epsilon_ending_value = 0.01
epsilon_decay_value = 0.995
epsilon = epsilon_starting_value
scores_on_100_episodes = deque(maxlen = 100)

for episode in range (1, number_episodes + 1):
  state, _ = env.reset()
  score = 0
  for t in range(maximum_number_timesteps_per_episode):
    action = agent.act(state, epsilon)
    next_state, reward, done, _, _ = env.step(action)
    agent.step(state, action, reward, next_state, done)
    state = next_state
    score += reward
    if done:
      break
  scores_on_100_episodes.append(score)
  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)
  print('\rEpisode{}\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = "")
  if episode % 100 == 0:
    print('\rEpisode{}\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))
  if np.mean(scores_on_100_episodes) >= 500.0:
    print('\n Environment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(episode -100, np.mean(scores_on_100_episodes)))
    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')
    break

"""## Part 3 - Visualizing the results"""

import glob
import io
import base64
import imageio
from IPython.display import HTML, display
from gym.wrappers.monitoring.video_recorder import VideoRecorder

def show_video_of_model(agent, env_name):
    env = gym.make(env_name, render_mode='rgb_array')
    state, _ = env.reset()
    done = False
    frames = []
    while not done:
        frame = env.render()
        frames.append(frame)
        action = agent.act(state)
        state, reward, done, _, _ = env.step(action)
    env.close()
    imageio.mimsave('video.mp4', frames, fps=30)

show_video_of_model(agent, 'MsPacmanDeterministic-v0')

def show_video():
    mp4list = glob.glob('*.mp4')
    if len(mp4list) > 0:
        mp4 = mp4list[0]
        video = io.open(mp4, 'r+b').read()
        encoded = base64.b64encode(video)
        display(HTML(data='''<video alt="test" autoplay
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
    else:
        print("Could not find video")

show_video()