# -*- coding: utf-8 -*-
"""Copy of A3C for Kung Fu - Partial Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/136vfW5Cbj_IlKdC-wx9ytMs0eOnjZS9D

# A3C for Kung Fu

## Part 0 - Installing the required packages and importing the libraries

### Installing Gymnasium
"""

!pip install gymnasium
!pip install "gymnasium[atari, accept-rom-license]"
!pip install ale-py
!apt-get install -y swig
!pip install gymnasium[box2d]

"""### Importing the libraries"""

import cv2
import math
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.multiprocessing as mp
import torch.distributions as distributions
from torch.distributions import Categorical
import ale_py
import gymnasium as gym
from gymnasium.spaces import Box
from gymnasium import ObservationWrapper

"""## Part 1 - Building the AI"""

class Network(nn.Module):

  def __init__(self, action_size):
        super(Network, self).__init__()

        # Convolutional layers for feature extraction
        self.conv1 = torch.nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(3, 3), stride=2)
        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2)
        self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2)

        # Flatten layer
        self.flatten = torch.nn.Flatten()

        # Fully connected layers
        self.fc1 = torch.nn.Linear(512, 128)  # Adjust this if input size changes
        self.fc2a = torch.nn.Linear(128, action_size)  # Action probabilities
        self.fc2s = torch.nn.Linear(128, 1)  # State value (scalar)

  def forward(self, state):
        # Pass through convolutional layers with ReLU activation
        x = F.relu(self.conv1(state))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        # Flatten the features for fully connected layers
        x = self.flatten(x)

        # Pass through the first fully connected layer with ReLU
        x = F.relu(self.fc1(x))

        # Separate into action values and state value
        action_values = self.fc2a(x)  # Policy
        state_value = self.fc2s(x)    # Critic (scalar output)

        return action_values, state_value

"""### Creating the architecture of the Neural Network

## Part 2 - Training the AI

### Setting up the environment
"""

class PreprocessAtari(ObservationWrapper):

  def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):
    super(PreprocessAtari, self).__init__(env)
    self.img_size = (height, width)
    self.crop = crop
    self.dim_order = dim_order
    self.color = color
    self.frame_stack = n_frames
    n_channels = 3 * n_frames if color else n_frames
    obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]
    self.observation_space = Box(0.0, 1.0, obs_shape)
    self.frames = np.zeros(obs_shape, dtype = np.float32)

  def reset(self):
    self.frames = np.zeros_like(self.frames)
    obs, info = self.env.reset()
    self.update_buffer(obs)
    return self.frames, info

  def observation(self, img):
    img = self.crop(img)
    img = cv2.resize(img, self.img_size)
    if not self.color:
      if len(img.shape) == 3 and img.shape[2] == 3:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = img.astype('float32') / 255.
    if self.color:
      self.frames = np.roll(self.frames, shift = -3, axis = 0)
    else:
      self.frames = np.roll(self.frames, shift = -1, axis = 0)
    if self.color:
      self.frames[-3:] = img
    else:
      self.frames[-1] = img
    return self.frames

  def update_buffer(self, obs):
    self.frames = self.observation(obs)

def make_env():
  env = gym.make("KungFuMasterDeterministic-v0", render_mode = 'rgb_array')
  env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)
  return env

env = make_env()

state_shape = env.observation_space.shape
number_actions = env.action_space.n
print("State shape:", state_shape)
print("Number actions:", number_actions)
print("Action names:", env.env.env.env.get_action_meanings())

"""### Initializing the hyperparameters"""

learning_rate = 1e-4
discount_factor = 0.99
number_environments = 10

"""### Implementing the A3C class"""

class Agent():
    def __init__(self, action_size):
        # Set the device to GPU if available, else default to CPU
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.action_size = action_size  # Number of possible actions
        # Initialize the neural network and move it to the appropriate device
        self.network = Network(action_size).to(self.device)
        # Use Adam optimizer to update the parameters of the network
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)

    def act(self, state):
        # Check if the state is not in a batch (3 dimensions instead of 4)
        if state.ndim == 3:
            # Add an extra batch dimension by wrapping the state in a list
            state = [state]

        # Convert the state from a NumPy array to a torch tensor
        # Specify the data type as float32 and move the tensor to the specified device
        state = torch.tensor(state, dtype=torch.float32, device=self.device)

        # Pass the state through the network to obtain the action values
        # The network forward method also outputs a state value, which is ignored here
        action_values, _ = self.network(state)

        # Apply the softmax function to the action values
        # This converts the action values into probabilities for each action
        policy = F.softmax(action_values, dim=-1)

        # Select an action based on the probabilities in the policy
        # For each policy (batch element), sample an action from the probability distribution
        return np.array([np.random.choice(len(p), p=p) for p in policy.detach().cpu().numpy()])

    def step(self, state, action, reward, next_state, done):
        # Get the batch size from the first dimension of the state tensor
        batch_size = state.shape[0]

        # Convert state, next_state, reward, and done into PyTorch tensors
        # Ensure proper data types and move tensors to the appropriate device (GPU/CPU)
        state = torch.tensor(state, dtype=torch.float32, device=self.device)
        next_state = torch.tensor(next_state, dtype=torch.float32, device=self.device)
        reward = torch.tensor(reward, dtype=torch.float32, device=self.device)
        # Convert done from boolean to float32 for compatibility in computations
        done = torch.tensor(done, dtype=torch.bool, device=self.device).to(dtype=torch.float32)

        # Get the action values and state value for the current state
        action_values, state_value = self.network(state)
        # Get the next state value (ignore the next action values using an underscore)
        _, next_state_value = self.network(next_state)

        # Compute the target state value using the Bellman equation
        # target_state_value = reward + gamma * next_state_value * (1 - done)
        target_state_value = reward + discount_factor * next_state_value * (1 - done)

        # Compute the advantage: how much better is the target state value compared to the predicted state value
        advantage = target_state_value - state_value

        # Compute the probabilities and log probabilities using softmax and log_softmax
        probs = F.softmax(action_values, dim=-1)  # Probabilities over actions
        logprobs = F.log_softmax(action_values, dim=-1)  # Log probabilities

        # Compute the entropy to encourage exploration
        # Entropy measures the uncertainty in the policy's action selection
        entropy = -torch.sum(probs * logprobs, axis=-1)

        # Create a batch index array to select actions from the batch
        batch_idx = np.arange(batch_size)
        # Select the log probabilities of the actions that were actually taken
        logp_actions = logprobs[batch_idx, action]

        # Compute the actor loss
        # Encourage actions with higher advantages while adding entropy for exploration
        actor_loss = -(logp_actions * advantage.detach()).mean() - 0.001 * entropy.mean()

        # Compute the critic loss
        # This is the mean squared error between the target and predicted state values
        critic_loss = F.mse_loss(target_state_value.detach(), state_value)

        # Combine actor and critic losses to get the total loss
        total_loss = actor_loss + critic_loss

        # Reset the optimizer's gradients to prevent accumulation from previous steps
        self.optimizer.zero_grad()

        # Perform backpropagation to compute gradients for the total loss
        total_loss.backward()

        # Update the neural network's weights using the optimizer
        self.optimizer.step()



"""### Initializing the A3C agent"""

agent = Agent(number_actions)

"""### Evaluating our A3C agent on a single episode"""

def evaluate(agent, env, n_episodes=1):
    # Initialize a list to store total rewards for each episode
    episodes_rewards = []

    # Loop over the number of episodes
    for _ in range(n_episodes):
        # Reset the environment to get the initial state
        # The reset method also returns additional info, which we disregard
        state, _ = env.reset()

        # Initialize the total reward for the current episode
        total_reward = 0

        # Start an infinite loop for the current episode
        while True:
            # Use the agent's act method to decide the next action based on the current state
            action = agent.act(state)

            # Perform the action in the environment
            # The step method returns the next state, reward, done (episode status), and additional info
            # We disregard the additional info with an underscore
            state, reward, done, info, _ = env.step(action[0])  # Extract action for non-batch evaluation

            # Accumulate the reward for the current episode
            total_reward += reward

            # Check if the episode is done
            # If yes, break out of the infinite loop
            if done:
                break

        # Append the total reward of the current episode to the list
        episodes_rewards.append(total_reward)

    # Return the list of total rewards for all evaluated episodes
    return episodes_rewards

"""### Testing multiple agents on multiple environments at the same time"""

class EnvBatch:

    def __init__(self, n_envs=10):
        # Initialize the class with the number of environments (default is 10)
        # Create a list of multiple environments using the make_env function
        self.envs = [make_env() for _ in range(n_envs)]

    def reset(self):
        # Initialize an empty list to store the reset states of all environments
        _states = []

        # Loop through each environment in the list of environments
        for env in self.envs:
            # Reset each environment and append its initial state (index 0) to the list
            _states.append(env.reset()[0])

        # Convert the list of initial states into a NumPy array and return it
        return np.array(_states)

    def step(self, actions):
        # Step through all environments simultaneously
        # Use the map function to convert the results into NumPy arrays for each environment
        # The zip function groups the outputs of each environment's step call
        next_states, rewards, dones, infos, _ = map(
            np.array,
            zip(*[env.step(a) for env, a in zip(self.envs, actions)])
        )

        # Iterate through all environments to check if any are done
        # If done, reset the corresponding environment and update its next state
        for i in range(len(self.envs)):
            if dones[i]:  # Check if the current environment is done
                # Reset the environment and update the next state
                next_states[i] = self.envs[i].reset()[0]

        # Return the results as NumPy arrays for compatibility with the rest of the code
        return next_states, rewards, dones, infos

"""### Training the A3C agent"""

import tqdm

env_batch = EnvBatch(number_environments)
batch_states = env_batch.reset()

with tqdm.trange(0, 3001) as progress_bar:
  for i in progress_bar:
    batch_actions = agent.act(batch_states)
    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)
    batch_rewards *= 0.001
    agent.step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)
    batch_states = batch_next_states
    if i % 1000 == 0:
      print("Average agent reward: ", np.mean(evaluate(agent, env, n_episodes = 10)))

"""## Part 3 - Visualizing the results"""

import glob
import io
import base64
import imageio
from IPython.display import HTML, display

def show_video_of_model(agent, env):
  state, _ = env.reset()
  done = False
  frames = []
  while not done:
    frame = env.render()
    frames.append(frame)
    action = agent.act(state)
    state, reward, done, _, _ = env.step(action[0])
  env.close()
  imageio.mimsave('video.mp4', frames, fps=30)

show_video_of_model(agent, env)

def show_video():
    mp4list = glob.glob('*.mp4')
    if len(mp4list) > 0:
        mp4 = mp4list[0]
        video = io.open(mp4, 'r+b').read()
        encoded = base64.b64encode(video)
        display(HTML(data='''<video alt="test" autoplay
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
    else:
        print("Could not find video")

show_video()