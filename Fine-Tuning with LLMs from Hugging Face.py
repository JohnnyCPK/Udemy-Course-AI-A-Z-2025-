# -*- coding: utf-8 -*-
"""Copy of Fine-Tuning LLMs with Hugging Face - Partial Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CTHMCXxMaTZXLrW4t03z2YZrk_vZ_vNy

# Fine-Tuning LLMs with Hugging Face

## Step 1: Installing and importing the libraries
"""

!pip uninstall accelerate peft bitsandbytes transformers trl -y
!pip install accelerate peft==0.13.2 bitsandbytes transformers trl==0.12.0

!pip install huggingface_hub

import torch
from trl import SFTTrainer
from peft import LoraConfig
from datasets import load_dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)

"""## Step 2: Loading the model"""

# Load the fine-tuned Llama model from the specified repository on Hugging Face.
llama_model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path="aboonaji/llama2finetune-v2",  # Path or name of the fine-tuned model.
    quantization_config=BitsAndBytesConfig(  # Configure quantization to optimize memory usage.
        load_in_4bit=True,  # Enable 4-bit quantization to reduce model size and memory footprint.
        bnb_4bit_compute_dtype=getattr(torch, "float16"),  # Use 16-bit floating-point precision for computations.
        bnb_4bit_quant_type="nf4"  # Specify the type of 4-bit quantization as normalized float 4 (nf4).
    )
)

# Disable the use of cached computations to optimize memory usage during inference.
llama_model.config.use_cache = False

# Set tensor parallelism configuration for model training or inference.
llama_model.config.pretraining_tp = 1  # Value of 1 indicates a specific parallelism setup for pretraining.

"""## Step 3: Loading the tokenizer"""

# Load the tokenizer for the fine-tuned Llama model from the specified repository on Hugging Face.
llama_tokenizer = AutoTokenizer.from_pretrained(
    pretrained_model_name_or_path="aboonaji/llama2finetune-v2",  # Path or name of the fine-tuned tokenizer.
    trust_remote_code=True  # Allow loading custom tokenizer code from the repository.
)

# Set the padding token to be the same as the end-of-sequence (eos) token.
# This ensures compatibility when padding sequences of different lengths.
llama_tokenizer.pad_token = llama_tokenizer.eos_token

# Specify that padding should be added on the right side of the sequences.
# This means shorter sequences will have padding tokens added at the end.
llama_tokenizer.padding_side = "right"

"""## Step 4: Setting the training arguments"""

# Create a configuration object for training parameters using the Hugging Face Trainer API.
training_arguments = TrainingArguments(
    output_dir="./results",  # Directory where the training results and checkpoints will be saved.
    per_device_train_batch_size=4,  # Number of training samples per batch for each device (e.g., GPU/CPU).
    max_steps=100  # Maximum number of training steps to perform before stopping.
)

"""## Step 5: Creating the Supervised Fine-Tuning trainer"""

# Create an SFTTrainer object to fine-tune the Llama model using specific configurations and dataset.
llama_sft_trainer = SFTTrainer(
    model=llama_model,  # The pre-trained Llama model to be fine-tuned.
    args=training_arguments,  # Training parameters defined using TrainingArguments.

    # Load the training dataset from a Hugging Face repository.
    train_dataset=load_dataset(
        path="aboonaji/wiki_medical_terms_llam2_format",  # Path to the dataset repository.
        split="train"  # Use the "train" split of the dataset for training.
    ),

    tokenizer=llama_tokenizer,  # The tokenizer associated with the Llama model.

    # Configuration for Parameter-Efficient Fine-Tuning (PEFT) using LoRA.
    peft_config=LoraConfig(
        task_type="CAUSAL_LM",  # Specify the task type as causal language modeling (text generation).
        r=64,  # Rank of the low-rank matrices used in LoRA.
        lora_alpha=16,  # Scaling factor for LoRA updates.
        lora_dropout=0.1  # Dropout rate to prevent overfitting during training.
    ),

    dataset_text_field="text"  # Specify the field in the dataset that contains the input text data.
)

"""## Step 6: Training the model"""

llama_sft_trainer.train()

"""## Step 7: Chatting with the model"""

# Initialize an empty string to hold the user's input prompt.
user_prompt = "Please tell me about Ascariasis"

# Create a text generation pipeline using the Hugging Face Transformers library.
text_generation_pipeline = pipeline(
    task="text-generation",  # Specify the task as text generation.
    model=llama_model,  # Use the fine-tuned Llama model for text generation.
    tokenizer=llama_tokenizer,  # Use the associated tokenizer for encoding and decoding text.
    max_length=300  # Set the maximum length of the generated text.
)

# Generate a model response using the pipeline.
# The input format includes special tokens (<s> and [INST]) to guide the model's behavior.
model_answer = text_generation_pipeline(f"<s>[INST] {user_prompt} [/INST]")

# Print the generated text from the model's response.
print(model_answer[0]['generated_text'])